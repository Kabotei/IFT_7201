# -*- coding: utf-8 -*-
"""exercices_9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vFvAFrLRE_oRfhlEjaWbC9s_xsSpT109
"""

import numpy
import random
import gym
import torch
from torch.functional import Tensor
from torch.nn.functional import mse_loss
from poutyne import Model

def set_random_seed(environment, seed):
    environment.seed(seed)
    environment.action_space.seed(seed)
    environment.observation_space.seed(seed)
    numpy.random.seed(seed)
    torch.manual_seed(seed)
    random.seed(seed)

"""# Batch $Q$-Learning

Implémentez une politique $\varepsilon$-greedy pour générer les trajectoires :
"""

def epsilon_greedy_policy(q_vals, action_space, epsilon):
    '''
    Selects an action according to an epsilon-greedy policy.

    q_vals: (2,) numpy array where q_val[i] represents the estimated q_value for action i.
    action_space : ActionSpace object from an OpenAI Gym environment.
    epsilon: (between 0 and 1) represents the odd of selecting the action randomly.
    If the action is not selected randomly, it is selected as the argmax of the q_vals.

    Returns an integer action.
    '''
    if numpy.random.rand() < epsilon:
        return action_space.sample()
    else:
        return numpy.argmax(q_vals)

"""Implémentez la fonction suivante pour calculer les cibles du réseau selon la formule du 1-step SARSA lors de la mise à jour de $\theta$ :"""

def get_targets(next_q_vals, rewards, terminal, gamma):
    '''
    Returns Q-Learning targets according to the 1-step SARSA lookahead formula,
    i.e. target_t = r_t + gamma * max(Q(s_t+1))

    If s_t was already terminal, then we only have target_t = r_t.

    next_q_vals: (batch_size, 2) numpy array representing the Q(s_t+1) values
    rewards: (batch_size,) numpy array representing the r_t values
    terminal: (batch_size,) boolean numpy array representing if s_t+1 was terminal
    gamma: float between 0 and 1

    Returns a (batch_size,) numpy array containing the 1-step lookahead targets.
    '''
    
    next_action_values_selected = numpy.max(next_q_vals, axis=-1)
    targets = rewards + gamma * next_action_values_selected * (1 - terminal)

    return targets

"""Implémenter un replay buffer qui entrepose un nombre fixe de transitions vues et à partir desquelles l'entraînement va se dérouler :"""

class ReplayBuffer:
    '''
    Replay buffer object that stores elements up until a certain maximum size.
    '''

    def __init__(self, buffer_size):
        '''
        Init the buffer and store buffer_size property.
        '''
        self.__list = []
        self.__max_size = buffer_size

    def store(self, element):
        '''
        Stores an element.

        If the buffer is already full, pop the oldest element inside.
        '''
        self.__list.append(element)

        if len(self.__list) > self.__max_size:
          del self.__list[0]

    def get_batch(self, batch_size):
        '''
        Randomly samples batch_size elements from the buffer.

        Returns the list of sampled elements.
        '''
        return random.sample(self.__list, batch_size)

    def __len__(self):
      return len(self.__list)

"""# Approximation de fonction linéaire

La fonction suivante permet de ré-initialiser tous les générateurs de nombres aléatoires pour pouvoir reproduire les expériences :

Considérons une approximation de fonction linéaire sur l'environnement CartPole-v1 de OpenAI Gym. On  a donc $f_\theta(s, a) = \langle \theta, \phi(s, a) \rangle$, le produit scalaire entre le vecteur de paramètres $\theta$ et la représentation vectorielle de la paire $(s, a)$. Pour exploiter la symétrie de l'environnement, prenons la représentation :
        
$$
\phi(s,a) =
    \begin{cases}
        s  & \quad\text{si } a = 1 \\
        -s & \quad\text{sinon.}
    \end{cases}
$$
"""


"""# Approximation de fonction neuronale

Regardons maintenant comment on peut apprendre une approximation pour la $Q$-function avec des réseaux de neurones !

On commmence par se créer une réseau de neurones. Pour les besoins du cours, on vous fournit une implémentation simple qui utilise la librairie de réseaux de neurones [Pytorch](https://pytorch.org) :
"""

class NNModel(torch.nn.Module):
    def __init__(self, in_dim, out_dim, n_hidden_layers=1, hidden_dim=32):
        '''
        Builds a PyTorch Neural Network with n_hidden_layers, all of hidden_dim neurons.

        The activation function is always ReLU for intermediate layers and the final 
        layer does not have any activation function.

        By default, this NN only has one hidden layer of 32 neurons.
        '''
        super().__init__()
        layers = []
        layers = [torch.nn.Linear(in_dim, hidden_dim), torch.nn.ReLU()]
        for _ in range(n_hidden_layers - 1):
            layers.extend([torch.nn.Linear(hidden_dim, hidden_dim), torch.nn.ReLU()])
        layers.append(torch.nn.Linear(hidden_dim, out_dim))

        self.fa = torch.nn.Sequential(*layers)

    def forward(self, x):
        '''
        This is the function that is called when we want to get the output f(x)
        for our NN f.
        '''
        return self.fa(x)

"""
On utilisera aussi la libraire [Poutyne](https://poutyne.org/), qui est essentiellement un wrapper autour de PyTorch, permettant de s'éviter d'avoir à réécrire toujours la même poutine de code PyTorch.  Voici quelques exemples des commandes à connaître par rapport à Poutyne :"""

environment = gym.make("CartPole-v1")
set_random_seed(environment, 42)

# A Poutyne model contains a neural network, the optimizer to use, and the loss function to minimize

# create a neural network
network = NNModel(environment.observation_space.shape[0], environment.action_space.n)

# optimizer: we pick Adam by default and give it a learning rate (lr) of 0.1
optim = torch.optim.Adam(network.parameters(), lr=1e-1)

# we select Mean Squared Error (MSE) as loss function
model_mse = Model(network, optim, loss_function="mse")

# get an initial observation to test our model
x = environment.reset()

# make the observation float32, which is the default type for PyTorch (used by Poutyne)
x = x.astype(numpy.float32)

# get predicted values by network
print(f"Q-valeurs pour l'état initial : {model_mse.predict_on_batch(x)}")

# for training, send inputs x and outputs y
# example: y is a vector of two ones
y = numpy.ones((2,))

# also make the target float32, still for PyTorch
y = y.astype(numpy.float32)
print(f"On utilisera la cible {y}")

# use function train_on_batch to update the neural network weights using the batch (x, y)
loss = model_mse.train_on_batch(x, y)
print(f"La perte pour la première batch est {loss}")

# predicted values at x are now closer to y
print(f"Q-valeurs pour l'état initial après la mise à jour : {model_mse.predict_on_batch(x)}")

"""Implémentez Batch $Q$-Learning (avec exploration $\varepsilon$-greedy) avec approximation neuronale sur CartPole-v1. Mettez à jour $\varepsilon$ en le mutipliant par un facteur de décroissance (epsilon_decay) après chaque trajectoire. Utilisez les valeurs par défaut fournies pour les autres paramètres. Lors de la génération de la trajectoire, entreposez dans le replay buffer un tuple contenant $s_t$, $a_t$, $r_{t+1}$, $s_{t+1}$, ainsi qu'un booléen indiquant si la trajectoire est terminée. Lorsque votre replay buffer contient au moins batch_size éléments, faites une mise à jour des poids à chaque pas de temps dans l'environnement."""

def dqn_loss(y_pred: Tensor, y_target: Tensor) -> Tensor:
    '''
    Input :
        - y_pred, (batch_size, n_actions) Tensor outputted by the network
        - y_target = (actions, Q_target), where actions and targets both
                      are Tensors with the shape (batch_size, ). 
                      Actions are the selected actions according to the target network
                      and targets are the one-step lookahead targets.

    Returns :
        - The DQN loss (same as for the linear case).
    '''
    
    actions, Q_target = y_target
    Q_predict = y_pred.gather(1, actions.unsqueeze(-1)).squeeze()

    return mse_loss(Q_predict, Q_target)


def deep_q_learning(
    gamma=0.99,
    n_trajectories=250,
    batch_size=128,
    lr=1e-3,
    buffer_size=50000,
    seed=42,
    epsilon_decay=0.98,
    epsilon_min=0.01,
):
    environment = gym.make("CartPole-v1")
    set_random_seed(environment, seed)
    
    # create a neural network model
    model = NNModel(environment.observation_space.shape[0], environment.action_space.n)

    # use Poutyne library, here with Adam optimizer (learning rate = 1e-3 and our own loss dqn_loss)
    agent = Model(model, torch.optim.Adam(model.parameters(), lr=lr), loss_function=dqn_loss)

    replay_buffer = ReplayBuffer(buffer_size)
    epsilon = 1.0

    for n_trajectories in range(n_trajectories):
        
        trajectory_done = False
        G = 0
        state = environment.reset().astype(numpy.float32)

        while not trajectory_done:
            # environment.render()

            q_values = agent.predict_on_batch(state)
            action = epsilon_greedy_policy(q_values, environment.action_space, epsilon)

            next_state, reward, trajectory_done, _ = environment.step(action)
            next_state = next_state.astype(numpy.float32)

            G += reward
            replay_buffer.store((state, action, reward, next_state, trajectory_done))

            state = next_state

            if len(replay_buffer) > batch_size:
                minibatch = replay_buffer.get_batch(batch_size)
                minibatch_columns = map(lambda x: numpy.array(x), zip(*minibatch))
                
                states, actions_taken, rewards, next_states, trajectories_done = minibatch_columns
            
                next_q_values = agent.predict_on_batch(next_states)
                targets = get_targets(next_q_values, rewards, trajectories_done, gamma).astype(numpy.float32)

                agent.train_on_batch(states, (actions_taken, targets))

        # environment.close()

        if (n_trajectories + 1) % 10 == 0:
            print(f"After {n_trajectories + 1} trajectories, we have G_0 = {G:.2f}, {epsilon:4f}")
    
        epsilon = max(epsilon * epsilon_decay, epsilon_min)

    return agent


agent = deep_q_learning(n_trajectories=250)

def run(agent):
    environment = gym.make("CartPole-v1")
    set_random_seed(environment, seed=42)

    done = False
    s = environment.reset().astype(numpy.float32)  # Important: cast as float32 for PyTorch
    while not done:
        environment.render()

        q_vals = agent.predict_on_batch(s)
        action = numpy.argmax(q_vals)
        next_s, r, done, _ = environment.step(action)
        s = next_s.astype(numpy.float32)  # Important: cast as float32 for PyTorch
    environment.close()

run(agent)
